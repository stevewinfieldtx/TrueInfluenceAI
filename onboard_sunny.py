"""
Onboard Sunny Lenarduzzi - Last 12 Months
============================================
Scans her YouTube channel, ingests videos from the past year,
runs temporal-weighted analysis, and outputs dashboard data.
"""

import requests
import json
import time
import sys
import io

# Fix Windows encoding for redirected output
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

API = "http://localhost:8100/api/v1"
COLLECTION_ID = "sunny"
CHANNEL_URL = "https://www.youtube.com/@SunnyLenarduzzi"

def api_post(path, data=None):
    r = requests.post(f"{API}{path}", json=data or {}, timeout=300)
    r.raise_for_status()
    return r.json()

def api_get(path, params=None):
    r = requests.get(f"{API}{path}", params=params, timeout=120)
    r.raise_for_status()
    return r.json()

def main():
    print("\n" + "="*60)
    print("  üöÄ ONBOARDING: Sunny Lenarduzzi")
    print("  Last 12 months of YouTube content")
    print("="*60)

    # 1. Create collection
    print("\n  üìÅ Creating collection...")
    try:
        col = api_post("/collections", {
            "collection_id": COLLECTION_ID,
            "template_id": "creator",
            "name": "Sunny Lenarduzzi",
            "description": "YouTube creator - last 12 months analysis",
            "metadata": {"channel_url": CHANNEL_URL}
        })
        print(f"  ‚úÖ Created: {col.get('name')}")
    except Exception as e:
        if "409" in str(e) or "already exists" in str(e).lower():
            print(f"  ‚è≠Ô∏è Collection exists, continuing...")
        else:
            print(f"  ‚ùå {e}")
            return

    # 2. Start channel ingestion (last 12 months = grab up to 100 videos, 
    #    the temporal analyzer will weight them properly)
    print(f"\n  üì∫ Starting channel scan: {CHANNEL_URL}")
    print(f"     Max videos: 100 (temporal weighting will handle recency)")
    
    try:
        result = api_post(f"/collections/{COLLECTION_ID}/ingest/youtube-channel", {
            "channel_url": CHANNEL_URL,
            "max_videos": 100,
            "min_duration": 60,
        })
        job_id = result.get("job_id")
        print(f"  ‚úÖ Ingestion job started: {job_id}")
    except Exception as e:
        print(f"  ‚ùå Channel ingestion failed: {e}")
        return

    # 3. Poll job progress
    if job_id:
        print(f"\n  ‚è≥ Ingesting videos (caption-first, this may take a few minutes)...\n")
        start = time.time()
        last_completed = 0
        
        while True:
            try:
                job = api_get(f"/jobs/{job_id}")
                status = job.get("status", "unknown")
                progress = job.get("progress", 0)
                completed = job.get("completed", 0)
                total = job.get("total", 0)
                errors = job.get("errors", 0)
                elapsed = int(time.time() - start)
                
                if completed != last_completed:
                    print(f"  [{elapsed:>4}s] {status:<12} {progress:>5.1f}% | {completed}/{total} videos | {errors} errors")
                    last_completed = completed
                
                if status in ("complete", "error", "failed"):
                    break
                    
                time.sleep(8)
                
            except Exception as e:
                print(f"  ‚ö†Ô∏è Poll error: {e}")
                time.sleep(10)
        
        elapsed = int(time.time() - start)
        print(f"\n  ‚úÖ Ingestion complete in {elapsed}s")
        print(f"     Videos: {completed}/{total} | Errors: {errors}")

    # 4. Check stats
    print(f"\n  üìä Collection stats...")
    stats = api_get(f"/collections/{COLLECTION_ID}/stats")
    print(f"     Sources: {stats.get('ready_sources')}")
    print(f"     Chunks:  {stats.get('chunk_count')}")
    print(f"     Hours:   {stats.get('total_duration_hours')}")

    # 5. Run analysis
    print(f"\n  üß† Running temporal-weighted analysis...")
    try:
        api_post(f"/collections/{COLLECTION_ID}/analyze")
    except Exception as e:
        print(f"  ‚ö†Ô∏è Analysis start: {e}")

    # Poll for analysis results
    print(f"  ‚è≥ Waiting for analysis to complete...")
    for attempt in range(30):
        time.sleep(10)
        try:
            analysis = api_get(f"/collections/{COLLECTION_ID}/analysis")
            if analysis and analysis.get("topics"):
                print(f"\n  ‚úÖ ANALYSIS COMPLETE!")
                
                # === TEMPORAL TRENDS ===
                trends = analysis.get("temporal_trends", {})
                
                print(f"\n  {'='*60}")
                print(f"  üìà CURRENT FOCUS (time-weighted)")
                print(f"  {'='*60}")
                
                current = analysis.get("current_focus", [])
                if current:
                    print(f"  Top topics RIGHT NOW: {', '.join(current[:8])}")
                
                # Show surging/rising
                surging = trends.get("surging_topics", [])
                rising = trends.get("rising_topics", [])
                if surging:
                    print(f"\n  üî• SURGING:")
                    for t in surging[:5]:
                        print(f"    ‚Üí {t['topic']} (velocity: {t['velocity']:.1f}/month)")
                if rising:
                    print(f"\n  üìà RISING:")
                    for t in rising[:5]:
                        print(f"    ‚Üí {t['topic']} (score: {t['trend_score']:.0f})")
                
                new_topics = trends.get("new_topics", [])
                if new_topics:
                    print(f"\n  üÜï NEW (only appeared recently):")
                    for t in new_topics[:5]:
                        print(f"    ‚Üí {t['topic']} (first: {t['first_seen']}, {t['recent_chunks']} chunks)")
                
                # Show declining/abandoned
                declining = trends.get("declining_topics", [])
                dormant = trends.get("dormant_topics", [])
                abandoned = trends.get("abandoned_topics", [])
                
                if declining:
                    print(f"\n  üìâ DECLINING:")
                    for t in declining[:5]:
                        print(f"    ‚Üí {t['topic']} (last: {t['last_seen']})")
                if dormant:
                    print(f"\n  üí§ DORMANT (6+ months silent):")
                    for t in dormant[:5]:
                        print(f"    ‚Üí {t['topic']} (was {t['historical']:.0f}% of content, last: {t['last_seen']})")
                if abandoned:
                    print(f"\n  ‚ùå ABANDONED (1+ year, was significant):")
                    for t in abandoned[:5]:
                        print(f"    ‚Üí {t['topic']} (was {t['historical']:.0f}% of content, last: {t['last_seen']})")
                
                # Focus shifts
                shifts = analysis.get("focus_shifts", [])
                if shifts:
                    print(f"\n  üîÑ FOCUS SHIFTS (pivots detected):")
                    for s in shifts[:5]:
                        print(f"    ‚Üí {s.get('from_topic')} ‚Üí {s.get('to_topic')}")
                        print(f"      {s.get('description', '')[:100]}")
                
                # === TOPIC TABLE ===
                topics = analysis.get("topics", [])
                if topics:
                    print(f"\n  {'='*60}")
                    print(f"  üìä ALL TOPICS (sorted by CURRENT weighted coverage)")
                    print(f"  {'='*60}")
                    print(f"  {'TOPIC':<28} {'NOW':>6} {'HIST':>6} {'TREND':<10} {'LAST SEEN':<12} {'CHUNKS':>6}")
                    print(f"  {'‚îÄ'*28} {'‚îÄ'*6} {'‚îÄ'*6} {'‚îÄ'*10} {'‚îÄ'*12} {'‚îÄ'*6}")
                    for t in topics[:25]:
                        name = t.get("topic", "?")[:27]
                        now = t.get("coverage_score", 0)
                        hist = t.get("historical_coverage", 0)
                        trend = t.get("trend", "?")
                        last = t.get("last_seen", "?")[:10]
                        chunks = t.get("chunk_count", 0)
                        
                        # Trend emoji
                        emoji = {"surging": "üî•", "rising": "üìà", "new": "üÜï",
                                 "stable": "‚û°Ô∏è", "declining": "üìâ", 
                                 "dormant": "üí§", "abandoned": "‚ùå"}.get(trend, "?")
                        
                        print(f"  {name:<28} {now:>5.1f}% {hist:>5.1f}% {emoji} {trend:<8} {last:<12} {chunks:>6}")
                
                # === GAPS ===
                gaps = analysis.get("gap_map", {})
                if gaps:
                    print(f"\n  {'='*60}")
                    print(f"  ‚ö†Ô∏è CONTENT GAPS (time-weighted)")
                    print(f"  {'='*60}")
                    for topic, score in list(gaps.items())[:10]:
                        bar = "‚ñà" * int(score / 5) + "‚ñë" * (20 - int(score / 5))
                        print(f"  {bar} {score:>5.0f}% {topic}")
                
                # === INSIGHTS ===
                insights = analysis.get("insights", [])
                if insights:
                    print(f"\n  {'='*60}")
                    print(f"  üí° AI INSIGHTS")
                    print(f"  {'='*60}")
                    for i, ins in enumerate(insights, 1):
                        pri = ins.get("priority", "?")
                        typ = ins.get("type", "?")
                        title = ins.get("title", "")
                        desc = ins.get("description", "")
                        action = ins.get("action", "")
                        print(f"\n  {i}. [{pri.upper()}|{typ}] {title}")
                        if desc:
                            print(f"     {desc[:120]}")
                        if action:
                            print(f"     ‚Üí {action[:120]}")
                
                # === TONE ===
                tone = analysis.get("tone_distribution", {})
                if tone:
                    print(f"\n  üé® TONE DISTRIBUTION (text-based only):")
                    for t, pct in sorted(tone.items(), key=lambda x: -x[1]):
                        bar = "‚ñà" * int(pct / 3)
                        print(f"    {bar} {pct:>5.1f}% {t}")
                
                # Save full analysis to file
                out_path = r"C:\Users\steve\Documents\TrueInfluenceAI\sunny_analysis.json"
                with open(out_path, "w") as f:
                    json.dump(analysis, f, indent=2)
                print(f"\n  üíæ Full analysis saved: {out_path}")
                
                print(f"\n  {'='*60}")
                print(f"  ‚úÖ SUNNY LENARDUZZI - ONBOARDING COMPLETE")
                print(f"  {'='*60}")
                return
                
        except Exception as e:
            pass
        
        print(f"    ... waiting ({(attempt+1)*10}s)")
    
    print(f"\n  ‚ö†Ô∏è Analysis timed out. Check API logs.")


if __name__ == "__main__":
    main()
